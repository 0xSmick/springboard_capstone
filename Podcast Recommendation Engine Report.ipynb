{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "With the growth of machine learning due to moore's law, I wanted to know if it were possible to be able to programmatically transcribe podcasts using open-source technology and use those transcriptions to fulfill two things, all within an application:\n",
    "\n",
    "* Provide me content recommendations based on episodes\n",
    "* Provide me content recommendations based on contextual search\n",
    "\n",
    "This notebook will provide an understanding of what was achiveved to accomplish this. \n",
    "\n",
    "For simplicity purposes we will breakdown each section of the pipeline and engineering process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business Concept\n",
    "\n",
    "The end goal for this capstone project was to create a real application that could be accessed by users. Podcasts have continued to grow at an alarming rate (more listeners than users on Twitter, a listener listens to 5 podcasts on average a week) but yet Apple has not innovated to capture this growth. The belief is with  a successful platform, we could capture a significant number of users and grow into a successful startup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Downloading the Source\n",
    "\n",
    "Retrieval of podcasts was no easy feat. The most difficult thing was finding where these podcasts were self-hosted. Fun fact, Apple actually doesn't own or host the podcats, and podcast creator needs to self host an xml file which points to the metadata as well as the podcast the location. The thought was if I could find teh actual podcast location, I could use python to download and store the podcasts. The following simple script worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting:  0  -  http://feeds.wnyc.org/radiolab\n",
      "0 :  http://feeds.wnyc.org/~r/radiolab/~5/sjLljunqqYg/radiolab111314.mp3\n",
      "downloading:  http://feeds.wnyc.org/~r/radiolab/~5/sjLljunqqYg/radiolab111314.mp3\n",
      "downloaded:  radiolab111314.mp3\n",
      "length:  1\n",
      "completed:  0  -  http://feeds.wnyc.org/radiolab\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib2\n",
    "import re\n",
    "import wget\n",
    "import os\n",
    "\n",
    "\n",
    "os.chdir('/Users/sheldon/git/springboard_capstone/acquire_podcasts')\n",
    "df = pd.read_csv('top100_pcasts_locations.csv')\n",
    "urls = df.url.tolist()\n",
    "urls = filter(lambda string: 'feeds.' in string or 'feed.' in string, urls)\n",
    "urls = urls[2:3]\n",
    "\n",
    "def homepage(request):\n",
    "    file = urllib2.urlopen(request)\n",
    "    data = file.read()\n",
    "    file.close()\n",
    "\n",
    "    def get_mp3s(data):\n",
    "        data = data.split()\n",
    "        data = filter(lambda word: word.endswith('.mp3\"') , data)\n",
    "        data = list(set(data))\n",
    "        return data\n",
    "    data = get_mp3s(data)  \n",
    "\n",
    "    def parse_mp3(urlstring):\n",
    "        urlstring = re.split('url=', urlstring)[1]\n",
    "        return urlstring.replace('\"','')\n",
    "    \n",
    "    data = map(parse_mp3, data)\n",
    "\n",
    "    return data\n",
    "\n",
    "def download_mp3(podcastseries, urls):\n",
    "    os.chdir('/Users/sheldon/git/springboard_capstone/acquire_podcasts')\n",
    "    os.mkdir(urls.split('/')[-1])\n",
    "    os.chdir(urls.split('/')[-1])\n",
    "    mp3_list = []\n",
    "    def download(episode):\n",
    "        print 'downloading: ',episode\n",
    "        episode = wget.download(episode)\n",
    "        print 'downloaded: ',episode\n",
    "\n",
    "    for number, episode in enumerate(podcastseries):\n",
    "        if len(mp3_list) < 1:\n",
    "            print number, ': ', episode\n",
    "            mp3_list.append(episode)\n",
    "            download(episode)\n",
    "            print 'length: ',len(mp3_list)\n",
    "        else:\n",
    "            break\n",
    "    os.chdir('/Users/sheldon/git/springboard_capstone/acquire_podcasts')\n",
    "    \n",
    "for number, series in enumerate(urls):\n",
    "    print 'starting: ',number, ' - ',series\n",
    "    data = homepage(series)\n",
    "    download_mp3(data, series)\n",
    "    print 'completed: ',number, ' - ',series\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look above you can see that we are accessing a csv file that has the locations of the podcasts, taking the html data and parsing it, to look for any words that end with \".mp3\" and then enacting our download_mp3 function to download each podcast into it's own folder. I orginally was downlaoding from the top 30 podcasts and transcrbing 11 episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Transcription"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing some research it appears that there is a fantastic plugin called pyAudio. PyAudio uses C based dependencies and can be linked to private API's or can be used with an open-source technology called CMU Sphinx. The detailed explanation of how CMU Sphinx is able to transcribe is not covered in this report as digital signal processing and audio processing is anything bu trivial, but for a quick version and extremely simple explanation. We are basically taking the mp3 files and transforming them to wav files. From the wav files we are able to split the file up by looking for silences, and then break down each ofm those words into 10 millisecond partitions. We use a Hidden Markov Model to do the recognition of the words. I highly recommend checking out http://cmusphinx.sourceforge.net/wiki/tutorialconcepts .\n",
    "\n",
    "Thankfully some wonderful researchers were able to place this into the library of speechRecognition, the code looks like the following (note this requires dependencies for install pockesphinx. You can do so with homebrew (brew install portaudio) pip install pyaudio, pip install speechRecognition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "import speech_recognition as sr\n",
    "r = sr.Recognizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something!\n"
     ]
    }
   ],
   "source": [
    "with sr.Microphone() as source:\n",
    "    print(\"Say something!\")\n",
    "    audio = r.listen(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing testing one two three\n",
      "CPU times: user 1.87 s, sys: 139 ms, total: 2.01 s\n",
      "Wall time: 2.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(r.recognize_sphinx(audio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you ran into problems running the code above, you will need to look into installing your dependencies: https://pypi.python.org/pypi/SpeechRecognition/\n",
    "\n",
    "The next step was comparison of the different options for transcription. I compared the open-source CMU vs Waston API. Since I decided to stick with CMU I will not be including the code to transcribe using Watson, but I did compare two samples to see what the drop off would be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk.data\n",
    "from __future__ import division  # Python 2 users only\n",
    "import nltk, re, pprint\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x10d7cf150>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXZIaYaBLUiMMdElqhIEuSY34f8RUAkX1iB14AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d7acbd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.chdir(\"/Users/sheldon/git/springboard_capstone/\")\n",
    "cmu_trans = open('report_assets/transcription_cmu.txt','rU').read()\n",
    "wat_trans = open('report_assets/transcription_watson_2.txt','rU').read()\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "## Tokenize and Lower the Words\n",
    "\n",
    "def tokenize_and_lower(textfile):\n",
    "    tokens = word_tokenize(textfile)\n",
    "    lower = [w.lower() for w in tokens]\n",
    "    filtered_words = [word for word in lower if word not in stop]\n",
    "    series = pd.Series(filtered_words)\n",
    "    return series\n",
    "\n",
    "## Compare results with value counts, presuming that Watson is more accurate than CMU\n",
    "\n",
    "cmu = tokenize_and_lower(cmu_trans)\n",
    "wat = tokenize_and_lower(wat_trans)\n",
    "cmu = pd.Series.to_frame(cmu)\n",
    "wat = pd.Series.to_frame(wat)\n",
    "cmu.columns = [['words']]\n",
    "wat.columns = [['words']]\n",
    "cmu = cmu.groupby('words').size().reset_index()\n",
    "wat = wat.groupby('words').size().reset_index()\n",
    "df = pd.merge(cmu, wat, on='words')\n",
    "df.columns = [['words','cmu','wat']]\n",
    "df['cmu_diff_wat'] = df.cmu - df.wat\n",
    "\n",
    "%matplotlib inline\n",
    "df.cmu_diff_wat.value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0    143\n",
       "-1     17\n",
       " 1     14\n",
       "-2      5\n",
       " 2      3\n",
       "-4      2\n",
       " 3      1\n",
       "-6      1\n",
       "Name: cmu_diff_wat, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cmu_diff_wat.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What the code above is doing, is taking the the small transcription of a text file, removing useless words (commonly known as stopwords, NLTK has a module for 2400 stopwords, and grouping by word then comparing the differences by taking the word counts for CMU and comparing that to Watson. 143 of the words had the same count, which is a measure of 76% (143 / 186). That was enough for me to go with CMU, as I could also parallelize CMU to run multiple podcasts at a time and not pay for API calls to transcribe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once I had my set of podcasts, I decided that I would first attempt to transcribe locally. This actaully took quite a long time as I ended up having issues with my mac falling asleep or the process crashing due to running out of memory. I finalized on separating out the files based on the series and individually running the transcription using the command line. This took a long time as I would run the script before work, after work and going to bed. Moving forward, as this project is something I am continuing to build out and use, I will be using Spark and AWS as an option for scaling transcription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Transform a full podcast I had to do several things:\n",
    "\n",
    "1. Take the mp3 and turn it into a wav. I used ffmpeg and the speech_recognition library\n",
    "2. Slice up each wav file into 300 seconds\n",
    "3. Transcribe those 300 seconds into a text file\n",
    "4. Save the transcription into an array of text files and concatenate the files together\n",
    "5. Remove the wav files as they take up a lot of memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#example code\n",
    "\n",
    "from pydub import AudioSegment\n",
    "import glob\n",
    "from math import ceil\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "import speech_recognition as sr\n",
    "r = sr.Recognizer()\n",
    "\n",
    "def transcribe_mp3(AUDIO_FILENAME, AUDIO_SEGMENT_SECONDS):\n",
    "    output_file_name = \"{}_translation.txt\".format(AUDIO_FILENAME)\n",
    "    #fuction to transform mp3 file to wav for transcription\n",
    "    try:\n",
    "        def transform_mp3_wav(AUDIO_FILENAME, AUDIO_SEGMENT_SECONDS):\n",
    "            filename = AUDIO_FILENAME.replace('.mp3','')\n",
    "            with open(AUDIO_FILENAME):\n",
    "                audio = AudioSegment.from_mp3(AUDIO_FILENAME)\n",
    "                xs = 0\n",
    "                while xs < audio.duration_seconds:\n",
    "                    ys = min(xs + AUDIO_SEGMENT_SECONDS, ceil(audio.duration_seconds))\n",
    "                    fname = str(xs).rjust(5, '0') + '-' + str(ys).rjust(5, '0') + '.wav'\n",
    "                    audio[xs*1000:ys*1000].export(os.getcwd() + '/' + filename + fname, format='wav')\n",
    "                    print(\"Saved\", fname)\n",
    "                    xs = ys\n",
    "        transform_mp3_wav(AUDIO_FILENAME, 300)\n",
    "        wav_filename = AUDIO_FILENAME.replace('.mp3','.wav')\n",
    "        wav_list = glob.glob('*.wav')\n",
    "        wav_list = filter(lambda x: '.mp3' not in x, wav_list)\n",
    "        trans_list = []\n",
    "        transcription = None\n",
    "        for wav_file in wav_list: \n",
    "            print 'transcribing: ' + wav_file\n",
    "            with sr.AudioFile(wav_file) as source:\n",
    "                audio = r.record(source)\n",
    "                transcription = r.recognize_sphinx(audio)\n",
    "                print 'transcription completed'\n",
    "            trans_list.extend(transcription)\n",
    "            \n",
    "        transcription = ''.join(trans_list)\n",
    "    except:\n",
    "        return 'error'\n",
    "    \n",
    "    for f in wav_list:\n",
    "        os.remove(f)\n",
    "    \n",
    "    file = open(output_file_name,'w')\n",
    "    file.write(transcription)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a lot of memory-errors and heart-ache I able to transcribe 150 podcasts from 11 different series. In the future I will be adding much more. I was able to take the transcriptions and write them to a no-sql database where I have the filepath, name of the series and the transcription. The dataframe is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def connect_db():\n",
    "\treturn sqlite3.connect('/Users/sheldon/podcasts/test.db')\n",
    "\n",
    "def create_df_object():\n",
    "\tconn = sqlite3.connect('/Users/sheldon/podcasts/test.db')\n",
    "\tdf = pd.read_sql(\"select * from podcast\",conn)\n",
    "\treturn df\n",
    "\n",
    "df = create_df_object()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 151 entries, 0 to 150\n",
      "Data columns (total 5 columns):\n",
      "index          151 non-null int64\n",
      "episode        151 non-null object\n",
      "locations      151 non-null object\n",
      "series         151 non-null object\n",
      "transcribed    151 non-null object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 6.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can tell it's a fairly small dataframe, but with scaling I will be able to make this a much higher potential (I have 3K podcasts ready to transcribe in an s3 bucket, but need to finish the spark code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Unsupervised Learning Models: TFIDF & Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step was looking how to create a recommendation engine based on the transcriptions of the podcast but not having a ground truth. It sounds like a perfect job for an unsupervised learning model. \n",
    "\n",
    "I originally started with using some simple NLP techniques which include TF-IDF and the cosine similarity.\n",
    "\n",
    "TF-IDF was a model that was originally used by Google for creating their pageranking engine with regards to content. It involves two simple calculations:\n",
    "* Term Frequency(t) = (Number of times term t appears in document)/(Total number of terms in the document)\n",
    "* Inverse Document Frequency(t) = log_e(Total number of documents)/(Number of documents with term t in it)\n",
    "\n",
    "Let's say I am looking for the word 'Clustering' and I have a document with 100 words and 'Clustering' appears in the document 3 times. TF = (3/100) = 0.03. If we had 1 million documen ts and the word clustering appears in 100 hundred of these then log(1 Million / 100) = 4. The TF-IDF weight is now 0.12.\n",
    "\n",
    "source:http://www.tfidf.com/\n",
    "\n",
    "So how can we use TF-IDF to create a recommendation engine, when each word actually has it's own TF-IDF weight? We will convert the documents to a document vector (or a vector space model) (an array of tf-idf numbers that show the value / importance of each word) and then take the dot product of the two vectors which results in transforming a vector into a scalar value (an actual number). We can then find the cosine similarity of the different documents. The measurement is one of orientation and not magnitude and we are able to use that to determine the angle and the 'similarity' of the documents. See the image below and this blog write up for more details: http://blog.christianperone.com/2013/09/machine-learning-cosine-similarity-for-vector-space-models-part-iii/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"http://blog.christianperone.com/wp-content/uploads/2013/09/cosinesimilarityfq1.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(url= \"http://blog.christianperone.com/wp-content/uploads/2013/09/cosinesimilarityfq1.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally I was using sci-kit learn and tfidf_vectorizer as my method for calcualting the cosine similarity. As per normal, sci-kit learn makes implementation of machine learning models fairly trivial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<151x35028 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 264535 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf = TfidfVectorizer(stop_words=stop)\n",
    "tfidf_matrix = tf.fit_transform(df['transcribed'])\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That creates a tfidf_matrix of podcast to terms (151 podcasts and 35028 terms). Then we create the cosine similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.59226052  0.54622254 ...,  0.54416222  0.55159127\n",
      "   0.58625829]\n",
      " [ 0.59226052  1.          0.53865688 ...,  0.55823367  0.55673508\n",
      "   0.5924624 ]\n",
      " [ 0.54622254  0.53865688  1.         ...,  0.50279116  0.49328254\n",
      "   0.52602526]\n",
      " ..., \n",
      " [ 0.54416222  0.55823367  0.50279116 ...,  1.          0.74494456\n",
      "   0.78448461]\n",
      " [ 0.55159127  0.55673508  0.49328254 ...,  0.74494456  1.          0.75725121]\n",
      " [ 0.58625829  0.5924624   0.52602526 ...,  0.78448461  0.75725121  1.        ]]\n",
      "151\n"
     ]
    }
   ],
   "source": [
    "cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "print cosine_similarities\n",
    "print len(cosine_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see above that we were able to use our linear kernel from scikit learn to transform the tfidf_matrix and get the cosine similarities for every podcast related to every podcast!\n",
    "\n",
    "Now we can put this into functions that can be usable for a user and future web app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_related_podcasts_scikit(podcast_number,number_of_similarities):\n",
    "    cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    related_pod_index = cosine_similarities.argsort()[podcast_number][::-1]\n",
    "    pod_dict = dict(zip(range(0, len(related_pod_index)),related_pod_index))\n",
    "    pod_dict = pd.DataFrame({'rank':pod_dict.keys()},index=pod_dict.values())\n",
    "    related_podcasts_df = pd.DataFrame.join(pod_dict, df, how='inner')\n",
    "    final_df = related_podcasts_df.sort_values('rank')[0:number_of_similarities+1][['rank','episode','series']]\n",
    "    return final_df\n",
    "\n",
    "def get_related_podcasts_query_scikit(query, number_of_similarities):\n",
    "    query = query.lower()\n",
    "    query = query.split()\n",
    "    tfidf_matrix_test = tf.fit_transform(query)\n",
    "    tfidf_matrix_train = tf.transform(df['transcribed'])\n",
    "    tfidf_matrix_train.todense()\n",
    "    tfidf_matrix_test.todense()\n",
    "    query_similarities = linear_kernel(tfidf_matrix_test, tfidf_matrix_train)\n",
    "    query_similarities = query_similarities.argsort()[0][::-1]\n",
    "    pod_dict = dict(zip(range(0, len(query_similarities)),query_similarities))\n",
    "    pod_dict = pd.DataFrame({'rank':pod_dict.keys()},index=pod_dict.values())\n",
    "    related_podcasts_df = pd.DataFrame.join(pod_dict, df, how='inner')\n",
    "    final_df = related_podcasts_df.sort_values('rank')[0:number_of_similarities+1][['rank','episode','series']]\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm creating two functions here:\n",
    "\n",
    "1. Retrieve the related podcasts based on the cosine similarity of that partciluar podcast compared to other podcasts.\n",
    "\n",
    "2. Retrieve the related podcasts based on the cosine similarity of a particular phrase. \n",
    "\n",
    "For 1, we are computing the cosine similarities, pulling the podcast based on the index, creating a dictionary of each index and ranking, joining that to original dataframe, sorting it by the rank, and returning the data frame.\n",
    "\n",
    "For 2, we are taking the query and doing some simple cleaning (lower and splitting into an array, transforming that array into a tfidf matrix, computing cosine similarity, and then copying the same thing as 1 with the the returned value being a dataframe. You can see the functions with their returned values below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>episode</th>\n",
       "      <th>series</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>0</td>\n",
       "      <td>snapjudgment012116_snap702_fullcircle.mp3_tran...</td>\n",
       "      <td>snapjudgment-wnyc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1</td>\n",
       "      <td>otm012916pod.mp3_translation.txt</td>\n",
       "      <td>onthemedia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>2</td>\n",
       "      <td>4399611-4-6-16-mark-levin-audio-rewind.mp3_tra...</td>\n",
       "      <td>MLAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>3</td>\n",
       "      <td>4245343-2-29-16-mark-levin-audio-rewind.mp3_tr...</td>\n",
       "      <td>MLAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>4</td>\n",
       "      <td>Pumped-on-Trump_podcast.mp3_translation.txt</td>\n",
       "      <td>revealpodcast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>5</td>\n",
       "      <td>4513041-5-2-16-mark-levin-audio-rewind.mp3_tra...</td>\n",
       "      <td>MLAR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rank                                            episode  \\\n",
       "105     0  snapjudgment012116_snap702_fullcircle.mp3_tran...   \n",
       "69      1                   otm012916pod.mp3_translation.txt   \n",
       "53      2  4399611-4-6-16-mark-levin-audio-rewind.mp3_tra...   \n",
       "50      3  4245343-2-29-16-mark-levin-audio-rewind.mp3_tr...   \n",
       "86      4        Pumped-on-Trump_podcast.mp3_translation.txt   \n",
       "54      5  4513041-5-2-16-mark-levin-audio-rewind.mp3_tra...   \n",
       "\n",
       "                series  \n",
       "105  snapjudgment-wnyc  \n",
       "69          onthemedia  \n",
       "53                MLAR  \n",
       "50                MLAR  \n",
       "86       revealpodcast  \n",
       "54                MLAR  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_related_podcasts_query_scikit('trump clinton obama guns',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>episode</th>\n",
       "      <th>series</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>freakonomics_podcast010213.mp3_translation.txt</td>\n",
       "      <td>freakonomics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>1</td>\n",
       "      <td>WS091715.mp3_translation.txt</td>\n",
       "      <td>wrestling_soup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>2</td>\n",
       "      <td>TIMFERRISSSHOW019.mp3_translation.txt</td>\n",
       "      <td>thetimferrissshow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>3</td>\n",
       "      <td>01 Ep. 12_ Judd Apatow.mp3_translation.txt</td>\n",
       "      <td>TheBillSimmonsPodcast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>4</td>\n",
       "      <td>WS030416.mp3_translation.txt</td>\n",
       "      <td>wrestling_soup</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>5</td>\n",
       "      <td>05-Tim_Ferriss_Show-Chase_Jarvis_128.mp3_trans...</td>\n",
       "      <td>thetimferrissshow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rank                                            episode  \\\n",
       "22      0     freakonomics_podcast010213.mp3_translation.txt   \n",
       "150     1                       WS091715.mp3_translation.txt   \n",
       "145     2              TIMFERRISSSHOW019.mp3_translation.txt   \n",
       "113     3         01 Ep. 12_ Judd Apatow.mp3_translation.txt   \n",
       "148     4                       WS030416.mp3_translation.txt   \n",
       "135     5  05-Tim_Ferriss_Show-Chase_Jarvis_128.mp3_trans...   \n",
       "\n",
       "                    series  \n",
       "22            freakonomics  \n",
       "150         wrestling_soup  \n",
       "145      thetimferrissshow  \n",
       "113  TheBillSimmonsPodcast  \n",
       "148         wrestling_soup  \n",
       "135      thetimferrissshow  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_related_podcasts_scikit(22,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 1.54 s per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit get_related_podcasts_query_scikit('economics math statistics',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 65.8 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit get_related_podcasts_scikit(22,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned before I originally implemented this model into the flask app, but noticed that for each query we had to recompute the cosine similarities, which wasn't sustainable. Thus I discovered Gensim, Topic Modeling for Humans."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Unsupervised Learning Models: Gensim & LSI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For anyone looking to play around with Gensim, I highly recommend the well documented API and the tutorials. Gensim, unlike scikit, actually has models that you can use to write to disk, which means the webserver could write the dictionary, corpus, and models all to disk and only need to read them once. Gensim also has a fantastic server module that the creator has just put out.\n",
    "\n",
    "For gensim, it was a similar process to setup:\n",
    "\n",
    "1. Clean the data (remove stopwords, capitalization, apostrophes)\n",
    "2. Create the Corpus (a collection of text documents, in this case the transcriptions)\n",
    "3. Create the TF-IDF Model\n",
    "4. Create the LSI Model\n",
    "5. Apply TFIDF on the corpus\n",
    "6. Apply LSI on the TFIDF Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSI (Also known as LSA) does not only find similarities based on the cosine of the angle, but additionally can extract out topics of any two documents using a method called Singular Value Decomposition, to determine the pattern of relationships between teh terms and the concepts in an unstructured collection of text (https://en.wikipedia.org/wiki/Latent_semantic_analysis).\n",
    "\n",
    "In translation we could feed in a list of podcasts transcriptions and be able to get the top related podcasts but also the themes which relate these podcasts.\n",
    "\n",
    "We are also able to overcome a big issue of boolean keyword queries, that have similar meanings and words that have more than one meaning (IE \"python internet\" might return a coding podcast but \"python jungle\" might return a travel podcast.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "os.chdir('/Users/sheldon/git/springboard_capstone/apps/')\n",
    "dictionary = gensim.corpora.Dictionary.load('models/words.dict')\n",
    "corpus = gensim.corpora.MmCorpus('models/corpus.mm')\n",
    "tfidf = gensim.models.tfidfmodel.TfidfModel.load('models/tfidf_model')\n",
    "lsi = gensim.models.lsimodel.LsiModel.load('models/model.lsi')\n",
    "index = gensim.similarities.MatrixSimilarity.load('models/corpus.index')\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "corpus_lsi = lsi[corpus_tfidf]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I loaded up all my models built off the podcast transcriptions, and built an LSI model with 50 different topics. Below is a sample of 10 of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'-0.068*\"republican\" + -0.053*\"movie\" + -0.050*\"conservative\" + -0.048*\"obama\" + -0.046*\"trump\" + -0.044*\"mortified\" + -0.044*\"health\" + -0.040*\"constitution\" + -0.040*\"government\" + -0.039*\"patients\"'),\n",
       " (1,\n",
       "  u'0.236*\"republican\" + 0.180*\"conservative\" + 0.169*\"trump\" + 0.155*\"constitution\" + 0.140*\"tromp\" + 0.138*\"obama\" + 0.127*\"donald\" + 0.127*\"immigration\" + 0.118*\"establishment\" + 0.097*\"crews\"'),\n",
       " (2,\n",
       "  u'-0.193*\"patients\" + -0.148*\"health\" + 0.123*\"j.\" + 0.109*\"mortified\" + -0.101*\"philadelphia\" + 0.096*\"cops\" + -0.096*\"patient\" + -0.093*\"obesity\" + 0.092*\"j\" + -0.089*\"research\"'),\n",
       " (3,\n",
       "  u'-0.374*\"taliban\" + -0.293*\"pakistan\" + -0.290*\"afghanistan\" + 0.123*\"mortified\" + -0.110*\"pakistani\" + -0.103*\"guantanamo\" + -0.096*\"j\" + -0.095*\"j.\" + -0.093*\"cops\" + -0.086*\"military\"'),\n",
       " (4,\n",
       "  u'-0.309*\"taliban\" + -0.247*\"afghanistan\" + -0.233*\"pakistan\" + 0.157*\"j.\" + 0.145*\"j\" + 0.140*\"cops\" + 0.107*\"thirteenth\" + 0.107*\"tower\" + 0.098*\"detectives\" + 0.098*\"jay\"'),\n",
       " (5,\n",
       "  u'-0.464*\"mortified\" + -0.155*\"diary\" + 0.102*\"game\" + -0.097*\"snakes\" + 0.089*\"games\" + -0.085*\"poetry\" + -0.079*\"utopia\" + -0.069*\"elizabeth\" + -0.067*\"vagina\" + 0.063*\"packers\"'),\n",
       " (6,\n",
       "  u'0.357*\"vulnerability\" + 0.155*\"shame\" + -0.121*\"patients\" + 0.101*\"shane\" + -0.093*\"health\" + 0.091*\"photography\" + 0.087*\"books\" + 0.086*\"courage\" + 0.084*\"etc\" + 0.079*\"vulnerable\"'),\n",
       " (7,\n",
       "  u'0.183*\"mortified\" + 0.117*\"game\" + 0.098*\"games\" + 0.097*\"vulnerability\" + -0.090*\"queens\" + 0.085*\"packers\" + 0.082*\"giants\" + 0.080*\"denver\" + 0.080*\"patients\" + 0.079*\"minus\"'),\n",
       " (8,\n",
       "  u'0.201*\"economics\" + -0.112*\"queens\" + -0.110*\"vulnerability\" + -0.098*\"patients\" + 0.097*\"economists\" + 0.087*\"gender\" + 0.081*\"offender\" + 0.069*\"coin\" + 0.069*\"offenders\" + 0.068*\"marijuana\"'),\n",
       " (9,\n",
       "  u'-0.373*\"movie\" + -0.257*\"movies\" + -0.109*\"hathaway\" + -0.093*\"bombed\" + -0.089*\"tribune\" + -0.089*\"academy\" + -0.089*\"bushel\" + -0.085*\"oscar\" + 0.079*\"vulnerability\" + 0.069*\"snap\"')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topics(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking through these topics you can immdiately see that there are some related themes, like 2 which has \"republican, conservative, and trump, constitution\". In fact, LSI has been tested by having a the machine cluster documents and having humans sort documents, and the results have been very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our topics we need to create similar functions from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_related_podcasts(index):\n",
    "    def getKey(item):\n",
    "        return item[1]\n",
    "    corpus = corpus_lsi[index]\n",
    "    corpus = sorted(corpus, key=getKey, reverse=True)[0:10]\n",
    "    related_df = pd.DataFrame(corpus,columns=['index','score'])\n",
    "    final_df = pd.merge(related_df, df, on='index')[['index','episode','score','series']]\n",
    "    return final_df\n",
    "\n",
    "def get_related_podcasts_query(query):\n",
    "    query = query.lower()\n",
    "    vec_box = dictionary.doc2bow(query.split())\n",
    "    vec_lsi = lsi[vec_box]\n",
    "    sims = index[vec_lsi]\n",
    "    sims = sorted(enumerate(sims), key=lambda item: -item[1])[0:10]\n",
    "    related_df = pd.DataFrame(sims,columns=['index','score'])\n",
    "    final_df = pd.merge(related_df, df, on='index')[['index','episode','score','series']]\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>episode</th>\n",
       "      <th>score</th>\n",
       "      <th>series</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>Episode_16_Poster_Boy.mp3_m_reK.tmp_translatio...</td>\n",
       "      <td>0.156565</td>\n",
       "      <td>CriminalShow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>Criminal.Ep14.FifthSuspectFINAL.mp3_translatio...</td>\n",
       "      <td>0.119628</td>\n",
       "      <td>CriminalShow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>dopequeens041916_cms596223_pod.mp3_translation...</td>\n",
       "      <td>0.102187</td>\n",
       "      <td>2DopeQueens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>dopequeens041216_cms593915_pod.mp3_translation...</td>\n",
       "      <td>0.094652</td>\n",
       "      <td>2DopeQueens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>Episode_28__P.D.I.D..mp3_translation.txt</td>\n",
       "      <td>0.074682</td>\n",
       "      <td>CriminalShow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>17</td>\n",
       "      <td>freakonomics_mppodcast071012.mp3_translation.txt</td>\n",
       "      <td>0.045444</td>\n",
       "      <td>freakonomics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>dopequeens050316_cms600651_pod.mp3_translation...</td>\n",
       "      <td>0.042427</td>\n",
       "      <td>2DopeQueens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>19</td>\n",
       "      <td>freakonomics_mppodcast103112.mp3_translation.txt</td>\n",
       "      <td>0.020345</td>\n",
       "      <td>freakonomics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20</td>\n",
       "      <td>freakonomics_mppodcast112812.mp3_translation.txt</td>\n",
       "      <td>0.016910</td>\n",
       "      <td>freakonomics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>freakonomics_mppodcast100511.mp3_translation.txt</td>\n",
       "      <td>0.011356</td>\n",
       "      <td>freakonomics</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            episode     score  \\\n",
       "0     11  Episode_16_Poster_Boy.mp3_m_reK.tmp_translatio...  0.156565   \n",
       "1      9  Criminal.Ep14.FifthSuspectFINAL.mp3_translatio...  0.119628   \n",
       "2      3  dopequeens041916_cms596223_pod.mp3_translation...  0.102187   \n",
       "3      2  dopequeens041216_cms593915_pod.mp3_translation...  0.094652   \n",
       "4     14           Episode_28__P.D.I.D..mp3_translation.txt  0.074682   \n",
       "5     17   freakonomics_mppodcast071012.mp3_translation.txt  0.045444   \n",
       "6      5  dopequeens050316_cms600651_pod.mp3_translation...  0.042427   \n",
       "7     19   freakonomics_mppodcast103112.mp3_translation.txt  0.020345   \n",
       "8     20   freakonomics_mppodcast112812.mp3_translation.txt  0.016910   \n",
       "9     18   freakonomics_mppodcast100511.mp3_translation.txt  0.011356   \n",
       "\n",
       "         series  \n",
       "0  CriminalShow  \n",
       "1  CriminalShow  \n",
       "2   2DopeQueens  \n",
       "3   2DopeQueens  \n",
       "4  CriminalShow  \n",
       "5  freakonomics  \n",
       "6   2DopeQueens  \n",
       "7  freakonomics  \n",
       "8  freakonomics  \n",
       "9  freakonomics  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_related_podcasts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>episode</th>\n",
       "      <th>score</th>\n",
       "      <th>series</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>53</td>\n",
       "      <td>4399611-4-6-16-mark-levin-audio-rewind.mp3_tra...</td>\n",
       "      <td>0.105772</td>\n",
       "      <td>MLAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>4019946-1-4-16-mark-levin-audio-rewind.mp3_tra...</td>\n",
       "      <td>0.093135</td>\n",
       "      <td>MLAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52</td>\n",
       "      <td>4351605-3-25-16-mark-levin-audio-rewind.mp3_tr...</td>\n",
       "      <td>0.087484</td>\n",
       "      <td>MLAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>4513041-5-2-16-mark-levin-audio-rewind.mp3_tra...</td>\n",
       "      <td>0.071596</td>\n",
       "      <td>MLAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50</td>\n",
       "      <td>4245343-2-29-16-mark-levin-audio-rewind.mp3_tr...</td>\n",
       "      <td>0.062403</td>\n",
       "      <td>MLAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>49</td>\n",
       "      <td>4205309-2-19-16-mark-levin-audio-rewind.mp3_tr...</td>\n",
       "      <td>0.053411</td>\n",
       "      <td>MLAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>86</td>\n",
       "      <td>Pumped-on-Trump_podcast.mp3_translation.txt</td>\n",
       "      <td>0.041023</td>\n",
       "      <td>revealpodcast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>47</td>\n",
       "      <td>4077439-1-18-16-mark-levin-audio-rewind.mp3_tr...</td>\n",
       "      <td>0.035475</td>\n",
       "      <td>MLAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>56</td>\n",
       "      <td>4579716-5-17-16-mark-levin-audio-rewind.mp3_tr...</td>\n",
       "      <td>0.025987</td>\n",
       "      <td>MLAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>55</td>\n",
       "      <td>4574251-5-16-16-mark-levin-audio-rewind.mp3_tr...</td>\n",
       "      <td>0.020811</td>\n",
       "      <td>MLAR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            episode     score  \\\n",
       "0     53  4399611-4-6-16-mark-levin-audio-rewind.mp3_tra...  0.105772   \n",
       "1     46  4019946-1-4-16-mark-levin-audio-rewind.mp3_tra...  0.093135   \n",
       "2     52  4351605-3-25-16-mark-levin-audio-rewind.mp3_tr...  0.087484   \n",
       "3     54  4513041-5-2-16-mark-levin-audio-rewind.mp3_tra...  0.071596   \n",
       "4     50  4245343-2-29-16-mark-levin-audio-rewind.mp3_tr...  0.062403   \n",
       "5     49  4205309-2-19-16-mark-levin-audio-rewind.mp3_tr...  0.053411   \n",
       "6     86        Pumped-on-Trump_podcast.mp3_translation.txt  0.041023   \n",
       "7     47  4077439-1-18-16-mark-levin-audio-rewind.mp3_tr...  0.035475   \n",
       "8     56  4579716-5-17-16-mark-levin-audio-rewind.mp3_tr...  0.025987   \n",
       "9     55  4574251-5-16-16-mark-levin-audio-rewind.mp3_tr...  0.020811   \n",
       "\n",
       "          series  \n",
       "0           MLAR  \n",
       "1           MLAR  \n",
       "2           MLAR  \n",
       "3           MLAR  \n",
       "4           MLAR  \n",
       "5           MLAR  \n",
       "6  revealpodcast  \n",
       "7           MLAR  \n",
       "8           MLAR  \n",
       "9           MLAR  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_related_podcasts_query(\"trump clinton obama guns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 loops, best of 3: 23.3 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit get_related_podcasts(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 5.51 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit get_related_podcasts_query('economics math statistics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you'd expect the speed is much faster without having to continuously do the cosine similarity computation. But also the results from an anecdotal perspective look much more accurate. MLAR is a popualr political podcast so it makes sense that it's been clustered in that politics topic. We can explore this further:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get list of related podcasts\n",
    "related_podcasts = list(get_related_podcasts(1)['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_topics_per_podcast(podcast_index):\n",
    "    def getKey(item):\n",
    "        return item[1]\n",
    "    topic_ids = [i for i in sorted(corpus_lsi[podcast_index], key=getKey, reverse=True) if i[1] > 0.10]\n",
    "    def get_topic_arrays(topic_ids):\n",
    "        x = []\n",
    "        for id in topic_ids:\n",
    "            list_of_words = sorted(lsi.show_topic(id[0], topn=5),key=getKey, reverse=True)\n",
    "            z = []\n",
    "            for word in list_of_words:\n",
    "                if word[1] > .05:\n",
    "                    z.append(word)\n",
    "            x.append(z)\n",
    "        return x\n",
    "    topic_arrays = get_topic_arrays(topic_ids)\n",
    "    return topic_arrays\n",
    "related_podcasts_topics_words = [[related_podcasts[i],get_topics_per_podcast(related_podcasts[i])] for i in range(0, len(related_podcasts))]\n",
    "episode_podcasts = list(get_related_podcasts(1)['episode'])\n",
    "series_podcasts = list(get_related_podcasts(1)['series'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Podcast: 1, ID: 11\n",
      "Podcast Series: CriminalShow\n",
      "Episode Title: Episode_16_Poster_Boy.mp3_m_reK.tmp_translation.txt\n",
      "topic: 0\n",
      "word: tommy, score:0.13349055465\n",
      "word: petrified, score:0.132104840356\n",
      "word: elkins, score:0.124858606537\n",
      "word: trump, score:0.112054639199\n",
      "topic: 1\n",
      "word: lakeview, score:0.162300386834\n",
      "word: chandler, score:0.133973881945\n",
      "topic: 2\n",
      "word: dean, score:0.15605057081\n",
      "word: police, score:0.0977091146594\n",
      "topic: 3\n",
      "word: movie, score:0.195235938874\n",
      "word: movies, score:0.134115893759\n",
      "word: assaulting, score:0.0923715665976\n",
      "Podcast: 2, ID: 9\n",
      "Podcast Series: CriminalShow\n",
      "Episode Title: Criminal.Ep14.FifthSuspectFINAL.mp3_translation.txt\n",
      "topic: 0\n",
      "word: tommy, score:0.13349055465\n",
      "word: petrified, score:0.132104840356\n",
      "word: elkins, score:0.124858606537\n",
      "word: trump, score:0.112054639199\n",
      "topic: 1\n",
      "word: tommy, score:0.13951262149\n",
      "word: lakeview, score:0.0894430773747\n",
      "topic: 2\n",
      "word: dean, score:0.15605057081\n",
      "word: police, score:0.0977091146594\n",
      "topic: 3\n",
      "word: vulnerability, score:0.133746491666\n",
      "word: queens, score:0.098773007173\n",
      "word: bushel, score:0.0975188733367\n",
      "word: offender, score:0.0933077193033\n",
      "word: sex, score:0.0848283443483\n",
      "topic: 4\n",
      "topic: 5\n",
      "word: economics, score:0.200752410305\n",
      "word: economists, score:0.096918993195\n",
      "topic: 6\n",
      "Podcast: 3, ID: 3\n",
      "Podcast Series: 2DopeQueens\n",
      "Episode Title: dopequeens041916_cms596223_pod.mp3_translation.txt\n",
      "topic: 0\n",
      "word: vulnerability, score:0.133746491666\n",
      "word: queens, score:0.098773007173\n",
      "word: bushel, score:0.0975188733367\n",
      "word: offender, score:0.0933077193033\n",
      "word: sex, score:0.0848283443483\n",
      "Podcast: 4, ID: 2\n",
      "Podcast Series: 2DopeQueens\n",
      "Episode Title: dopequeens041216_cms593915_pod.mp3_translation.txt\n",
      "topic: 0\n",
      "word: vulnerability, score:0.133746491666\n",
      "word: queens, score:0.098773007173\n",
      "word: bushel, score:0.0975188733367\n",
      "word: offender, score:0.0933077193033\n",
      "word: sex, score:0.0848283443483\n",
      "topic: 1\n",
      "word: mortified, score:0.123149227452\n",
      "Podcast: 5, ID: 14\n",
      "Podcast Series: CriminalShow\n",
      "Episode Title: Episode_28__P.D.I.D..mp3_translation.txt\n",
      "topic: 0\n",
      "word: gender, score:0.124241739699\n",
      "word: police, score:0.0942048275943\n",
      "word: caleb, score:0.0917341462318\n",
      "topic: 1\n",
      "word: vulnerability, score:0.133746491666\n",
      "word: queens, score:0.098773007173\n",
      "word: bushel, score:0.0975188733367\n",
      "word: offender, score:0.0933077193033\n",
      "word: sex, score:0.0848283443483\n",
      "topic: 2\n",
      "word: economics, score:0.200752410305\n",
      "word: economists, score:0.096918993195\n",
      "topic: 3\n",
      "word: movie, score:0.195235938874\n",
      "word: movies, score:0.134115893759\n",
      "word: assaulting, score:0.0923715665976\n",
      "topic: 4\n",
      "Podcast: 6, ID: 17\n",
      "Podcast Series: freakonomics\n",
      "Episode Title: freakonomics_mppodcast071012.mp3_translation.txt\n",
      "topic: 0\n",
      "word: economics, score:0.200752410305\n",
      "word: economists, score:0.096918993195\n",
      "Podcast: 7, ID: 5\n",
      "Podcast Series: 2DopeQueens\n",
      "Episode Title: dopequeens050316_cms600651_pod.mp3_translation.txt\n",
      "topic: 0\n",
      "word: vulnerability, score:0.133746491666\n",
      "word: queens, score:0.098773007173\n",
      "word: bushel, score:0.0975188733367\n",
      "word: offender, score:0.0933077193033\n",
      "word: sex, score:0.0848283443483\n",
      "Podcast: 8, ID: 19\n",
      "Podcast Series: freakonomics\n",
      "Episode Title: freakonomics_mppodcast103112.mp3_translation.txt\n",
      "topic: 0\n",
      "word: j., score:0.120245693603\n",
      "word: jenny, score:0.120199111096\n",
      "word: offender, score:0.106725105689\n",
      "topic: 1\n",
      "word: kidney, score:0.113627216132\n",
      "word: restrooms, score:0.0892374184549\n",
      "word: samsung, score:0.0890701474384\n",
      "topic: 2\n",
      "word: tommy, score:0.13951262149\n",
      "word: lakeview, score:0.0894430773747\n",
      "Podcast: 9, ID: 20\n",
      "Podcast Series: freakonomics\n",
      "Episode Title: freakonomics_mppodcast112812.mp3_translation.txt\n",
      "topic: 0\n",
      "word: economics, score:0.200752410305\n",
      "word: economists, score:0.096918993195\n",
      "topic: 1\n",
      "word: lakeview, score:0.127955210143\n",
      "word: caleb, score:0.108465881987\n",
      "topic: 2\n",
      "word: tommy, score:0.13951262149\n",
      "word: lakeview, score:0.0894430773747\n",
      "Podcast: 10, ID: 18\n",
      "Podcast Series: freakonomics\n",
      "Episode Title: freakonomics_mppodcast100511.mp3_translation.txt\n",
      "topic: 0\n",
      "word: lakeview, score:0.127955210143\n",
      "word: caleb, score:0.108465881987\n",
      "topic: 1\n",
      "word: economics, score:0.200752410305\n",
      "word: economists, score:0.096918993195\n",
      "topic: 2\n",
      "word: obesity, score:0.138132543449\n",
      "word: samsung, score:0.130565906525\n",
      "word: offender, score:0.129206697985\n",
      "word: polly, score:0.111471727217\n",
      "topic: 3\n",
      "word: offender, score:0.123822057248\n",
      "word: vulnerability, score:0.112299760848\n",
      "word: offenders, score:0.107192382584\n",
      "topic: 4\n",
      "word: j., score:0.120245693603\n",
      "word: jenny, score:0.120199111096\n",
      "word: offender, score:0.106725105689\n"
     ]
    }
   ],
   "source": [
    "for i,k in enumerate(related_podcasts_topics_words):\n",
    "    print \"Podcast: {}, ID: {}\".format(i+1, k[0])\n",
    "    print \"Podcast Series: {}\".format(series_podcasts[i])\n",
    "    print \"Episode Title: {}\".format(episode_podcasts[i])\n",
    "    for num, topic in enumerate(k[1]):\n",
    "        print \"topic: {}\".format(num)\n",
    "        for word in topic:\n",
    "            print \"word: {}, score:{}\".format(word[0], word[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking through the print out, you can start to piece how episodes are related to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Part V: Next Steps\n",
    "\n",
    "That about wraps up my capstone project. If you go to my repo you can clone it and try running the application. I am also trying to figure out how to push my flask app to heroku."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
